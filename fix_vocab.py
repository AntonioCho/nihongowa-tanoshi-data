# fix_vocab_main.py
import json
import re
import os
# ë¶„ë¦¬ëœ ë”•ì…”ë„ˆë¦¬ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
from fix_vocab_dict import MANUAL_MEANING_FIXES, MANUAL_POS_FIXES

INPUT_FILE = 'vocabData.json'
OUTPUT_FILE = 'vocabData_fixed.json'

# í˜•ìš©ì‚¬ ì–´ë¯¸(~ë‹¤, ~ì´ë‹¤)ë¥¼ ê´€í˜•ì–´(~í•œ, ~ì¸)ë¡œ ìë™ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def fix_adjective_korean(text):
    if not text:
        return text
    
    # ì—¬ëŸ¬ ëœ»ì´ ì½¤ë§ˆë¡œ ì—°ê²°ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬
    meanings = [m.strip() for m in text.split(',')]
    fixed_meanings = []
    
    # ë¹ˆë²ˆí•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ë¶ˆê·œì¹™ í˜•ìš©ì‚¬ ë³€í™˜ ì‚¬ì „ (í•˜ë“œì½”ë”©)
    exact_mapping = {
        "ì¢‹ë‹¤": "ì¢‹ì€", "ë§ë‹¤": "ë§ì€", "ì ë‹¤": "ì ì€", "í¬ë‹¤": "í°", "ì‘ë‹¤": "ì‘ì€",
        "ë†’ë‹¤": "ë†’ì€", "ë‚®ë‹¤": "ë‚®ì€", "ê¹Šë‹¤": "ê¹Šì€", "ì–•ë‹¤": "ì–•ì€", "ì¢ë‹¤": "ì¢ì€",
        "ë„“ë‹¤": "ë„“ì€", "ë©€ë‹¤": "ë¨¼", "ê°™ë‹¤": "ê°™ì€", "ì•„ë¦„ë‹µë‹¤": "ì•„ë¦„ë‹¤ìš´",
        "ì°¨ë‹¤": "ì°¬", "ê¸¸ë‹¤": "ê¸´", "ì§§ë‹¤": "ì§§ì€", "ì Šë‹¤": "ì Šì€", "ëŠ™ë‹¤": "ëŠ™ì€",
        "ê·€ì—½ë‹¤": "ê·€ì—¬ìš´", "ë‘¥ê¸€ë‹¤": "ë‘¥ê·¼", "ì‹œë‹¤": "ì‹ ", "ì“°ë‹¤": "ì“´", "ë‹¬ë‹¤": "ë‹¨", 
        "ì§œë‹¤": "ì§ ", "ë§µë‹¤": "ë§¤ìš´", "ë¥ë‹¤": "ë”ìš´", "ì¶¥ë‹¤": "ì¶”ìš´", "ë¬´ê²ë‹¤": "ë¬´ê±°ìš´",
        "ê°€ë³ë‹¤": "ê°€ë²¼ìš´", "ë¬´ì„­ë‹¤": "ë¬´ì„œìš´", "ì•„í”„ë‹¤": "ì•„í”ˆ", "ê¸°ì˜ë‹¤": "ê¸°ìœ", "ìŠ¬í”„ë‹¤": "ìŠ¬í”ˆ",
        "ë¹ ë¥´ë‹¤": "ë¹ ë¥¸", "ëŠë¦¬ë‹¤": "ëŠë¦°", "ë‹¤ë¥´ë‹¤": "ë‹¤ë¥¸", "ì¬ë¯¸ìˆë‹¤": "ì¬ë¯¸ìˆëŠ”", "ë§›ìˆë‹¤": "ë§›ìˆëŠ”",
        "ë§›ì—†ë‹¤": "ë§›ì—†ëŠ”", "ì–´ë µë‹¤": "ì–´ë ¤ìš´", "ì‰½ë‹¤": "ì‰¬ìš´"
    }

    for m in meanings:
        if m in exact_mapping:
            fixed_meanings.append(exact_mapping[m])
            continue
            
        original_m = m
        
        # ì •ê·œì‹ì„ ì´ìš©í•œ ì¼ê´„ ì–´ë¯¸ ì¹˜í™˜
        m = re.sub(r'í•˜ë‹¤$', 'í•œ', m)
        m = re.sub(r'ì´ë‹¤$', 'ì¸', m)
        m = re.sub(r'ìŠ¤ëŸ½ë‹¤$', 'ìŠ¤ëŸ¬ìš´', m)
        m = re.sub(r'ë¡­ë‹¤$', 'ë¡œìš´', m)
        m = re.sub(r'([ê°€-í£])ã…‚ë‹¤$', r'\1ìš´', m) # ã…‚ë¶ˆê·œì¹™ (ì˜ˆ: ë¶€ë“œëŸ½ë‹¤ -> ë¶€ë“œëŸ¬ìš´)
        m = re.sub(r'ì˜ë‹¤$', 'ìœ', m)           
        m = re.sub(r'í”„ë‹¤$', 'í”ˆ', m)           
        m = re.sub(r'ë¥´ë‹¤$', 'ë¥¸', m)           
        m = re.sub(r'ê¸°ë‹¤$', 'ê¸´', m)           
        m = re.sub(r'ìˆë‹¤$', 'ìˆëŠ”', m)         
        m = re.sub(r'ì—†ë‹¤$', 'ì—†ëŠ”', m)         
        m = re.sub(r'([ê°€-í£])ë‹µë‹¤$', r'\1ë‹¤ìš´', m)
        
        # ìœ„ ê·œì¹™ì— ê±¸ë¦¬ì§€ ì•Šì€ ì¼ë°˜ì ì¸ 'ë‹¤'ë¡œ ëë‚˜ëŠ” í˜•ìš©ì‚¬ì˜ ê²½ìš° (ë‹¨, '~ë³´ë‹¤' ë“± ì˜ˆì™¸ ë°©ì§€)
        if m == original_m and m.endswith('ë‹¤') and not m.endswith('ë³´ë‹¤'):
            m = re.sub(r'ë‹¤$', 'ì€', m)
            
        fixed_meanings.append(m)
        
    return ", ".join(fixed_meanings)

def process_vocabulary():
    if not os.path.exists(INPUT_FILE):
        print(f"ì˜¤ë¥˜: '{INPUT_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ í´ë”ì— ìœ„ì¹˜ì‹œì¼œì£¼ì„¸ìš”.")
        return

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        json_data = json.load(f)

    # ë°ì´í„° ê³„ì¸µ ìˆœíšŒ: data -> Level -> Chapter -> Vocabulary List
    base_data = json_data.get("data", {})
    
    modified_count = 0
    adj_fixed_count = 0

    for level, chapters in base_data.items():
        for chapter, vocab_list in chapters.items():
            for vocab in vocab_list:
                kanji = vocab.get("kanji", "")
                
                # 1. ìˆ˜ë™ ì§€ì •ëœ ì¹˜ëª…ì  ì˜¤ì—­ ë° ë¬¸ë§¥ ì˜¤ë¥˜ êµì • ì ìš©
                if kanji in MANUAL_MEANING_FIXES:
                    fixes = MANUAL_MEANING_FIXES[kanji]
                    if "meaning" in fixes:
                        vocab["meaning"] = fixes["meaning"]
                    if "meaningKo" in fixes:
                        vocab["meaningKo"] = fixes["meaningKo"]
                    modified_count += 1
                
                # 2. ì˜ëª» ì§€ì •ëœ í’ˆì‚¬ êµì • ì ìš©
                if kanji in MANUAL_POS_FIXES:
                    vocab["parts"] = MANUAL_POS_FIXES[kanji]
                    modified_count += 1

                # 3. í˜•ìš©ì‚¬ í•œê¸€ ëœ» ëë§ºìŒ ìë™ ë³€í™˜ (~ë‹¤ -> ~í•œ ë“±)
                if "adjective" in vocab.get("parts", "").lower():
                    original_ko = vocab.get("meaningKo", "")
                    fixed_ko = fix_adjective_korean(original_ko)
                    
                    if original_ko != fixed_ko:
                        vocab["meaningKo"] = fixed_ko
                        adj_fixed_count += 1

    # ë³€ê²½ëœ JSON ë°ì´í„°ë¥¼ ìƒˆ íŒŒì¼ë¡œ ì €ì¥ (í•œê¸€ ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ ensure_ascii=False ì„¤ì •)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    print("="*50)
    print("ğŸ‰ ë‹¨ì–´ì¥ ìë™ êµì • ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"- ë‹¤ì˜ì–´/ì˜¤ì—­ ë° í’ˆì‚¬ ìˆ˜ë™ êµì • ê±´ìˆ˜: {modified_count}ê±´")
    print(f"- í˜•ìš©ì‚¬ ì–´ë¯¸(~ë‹¤ -> ~í•œ) ìë™ ë³€í™˜ ê±´ìˆ˜: {adj_fixed_count}ê±´")
    print(f"- ê²°ê³¼ íŒŒì¼: '{OUTPUT_FILE}'ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*50)

if __name__ == "__main__":
    process_vocabulary()